name: SF Grid — scrape + publish

on:
  schedule:
    - cron: "0 */2 * * *"     # every 2 hours
  workflow_dispatch: {}

permissions:
  contents: write
  pull-requests: write
  pages: write
  id-token: write

concurrency:
  group: sfcompute-data
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install requirements
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt

      # (Optional) Pre-install CmdStan toolchain once to reduce Prophet’s first-fit cost
      # You can keep/omit this; it adds a few minutes on a cold runner.
      - name: Prepare cmdstan (optional)
        run: |
          python - <<'PY'
          try:
              from cmdstanpy import install_cmdstan
              install_cmdstan()
              print("CmdStan installed.")
          except Exception as e:
              print("CmdStan installation skipped/failed:", e)
          PY

      - name: Run scraper notebook (no in-place edits)
        run: |
          jupyter nbconvert --to notebook --execute sf_grid_scrape.ipynb \
          --output sf_grid_scrape_executed --output-dir .

      # --- Build dashboard data (history + latest) ---
      - name: Build dashboard data
        run: |
          set -euo pipefail
          mkdir -p docs/data
          python - <<'PY'
          import glob, pandas as pd
          files = sorted(glob.glob("sfcompute_grid/*.csv"))
          if not files:
              raise SystemExit("No CSV files found in sfcompute_grid/")
          hist = pd.concat((pd.read_csv(f) for f in files), ignore_index=True).drop_duplicates()
          hist.to_csv("docs/data/sfcompute_history.csv", index=False)
          pd.read_csv(files[-1]).to_csv("docs/data/sfcompute_latest.csv", index=False)
          print(f"Aggregated {len(files)} files into docs/data/sfcompute_history.csv")
          PY

      # --- Commit + rebase + push (single atomic step) ---
      - name: Commit and push updated data
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
          BRANCH: main
        run: |
          set -e
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git checkout "$BRANCH"

          # Stage everything produced by this run
          git add sfcompute_grid/*.csv docs/data/sfcompute_*.csv || true

          # Nothing new? Exit cleanly.
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          # Commit FIRST (clean index), THEN rebase and push
          git commit -m "Update SF grid data: $(date -u +'%Y-%m-%dT%H:%M:%SZ') [skip ci]"

          # Rebase our commit(s) on top of latest remote
          git pull --rebase "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}.git" "$BRANCH"

          # Push rebased head
          git push "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}.git" "HEAD:${BRANCH}"











