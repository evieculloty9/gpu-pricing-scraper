name: SF Grid â€” scrape + publish

on:
  schedule:
    - cron: "7 7 * * *"      # runs daily at 07:07 UTC
  workflow_dispatch: {}       # allow manual run

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas jupyter nbconvert
          # If your scraper needs extra libs, add them here
          # e.g. Playwright:
          # pip install playwright
          # python -m playwright install --with-deps chromium

      # If your scraper is a notebook, execute it:
      - name: Run scraper notebook
        run: |
          jupyter nbconvert --to notebook --execute --inplace sf_grid_scrape.ipynb

      # Build/refresh the dashboard datasets
      - name: Build dashboard data
        run: |
          mkdir -p docs/data
          python - <<'PY'
import pandas as pd, glob, os
files = sorted(glob.glob("sfcompute_grid/*.csv"))
if not files:
    raise SystemExit("No CSV files found in sfcompute_grid/")
dfs = [pd.read_csv(f) for f in files]
hist = pd.concat(dfs, ignore_index=True).drop_duplicates()
hist.to_csv("docs/data/sfcompute_history.csv", index=False)
pd.read_csv(files[-1]).to_csv("docs/data/sfcompute_latest.csv", index=False)
print("Wrote docs/data/sfcompute_latest.csv and docs/data/sfcompute_history.csv")
PY

      - name: Commit dashboard & data
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add docs/data/*.csv docs/index.html
          git commit -m "Publish SFCompute dashboard data" || echo "No changes"
          git push
