name: SF Grid â€” scrape + publish

on:
  schedule:
    - cron: "0 */2 * * *"     # every 2 hours
  workflow_dispatch: {}

permissions:
  contents: write

# Avoid two runs writing at once (race -> non-fast-forward)
concurrency:
  group: sfcompute-data
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0            # we need full history for pull/rebase

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas jupyter nbconvert
          # pip install playwright
          # python -m playwright install --with-deps chromium

      - name: Run scraper notebook
        run: |
          jupyter nbconvert --to notebook --execute --inplace sf_grid_scrape.ipynb

      # --- Commit raw timestamped CSVs ---
      - name: Commit raw scrape CSVs
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
          BRANCH: ${{ github.ref_name }}
        run: |
          set -e
          git config user.name  "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add sfcompute_grid/*.csv || true
          # If nothing to add, stop early
          if git diff --cached --quiet; then
            echo "No new raw CSVs to commit."
          else
            git commit -m "Add raw scrape CSV(s)"
            # sync with remote before pushing to avoid non-fast-forward
            git pull --rebase "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}.git" "${BRANCH}"
            git push  "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}.git" "HEAD:${BRANCH}"
          fi

      # --- Build dashboard data (history + latest) ---
      - name: Build dashboard data
        run: |
          set -euo pipefail
          mkdir -p docs/data
          python - <<'PY'
          import glob, pandas as pd
          files = sorted(glob.glob("sfcompute_grid/*.csv"))
          if not files:
              raise SystemExit("No CSV files found in sfcompute_grid/")
          hist = pd.concat((pd.read_csv(f) for f in files), ignore_index=True).drop_duplicates()
          hist.to_csv("docs/data/sfcompute_history.csv", index=False)
          pd.read_csv(files[-1]).to_csv("docs/data/sfcompute_latest.csv", index=False)
          print(f"Aggregated {len(files)} files into docs/data/sfcompute_history.csv")
          PY

      # --- Commit dashboard CSVs ---
      - name: Commit dashboard data
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
          BRANCH: ${{ github.ref_name }}
        run: |
          set -e
          git config user.name  "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add docs/data/*.csv || true
          if git diff --cached --quiet; then
            echo "No dashboard changes."
          else
            git commit -m "Update dashboard data"
            git pull --rebase "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}.git" "${BRANCH}"
            git push  "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}.git" "HEAD:${BRANCH}"
          fi





