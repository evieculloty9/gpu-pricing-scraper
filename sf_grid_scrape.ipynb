{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (1.48.0)\n",
      "Requirement already satisfied: pandas in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: pytz in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (2020.1)\n",
      "Requirement already satisfied: greenlet==3.1.1 in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: pyee==12.0.0 in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (from playwright) (12.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (from pyee==12.0.0->playwright) (4.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/evieculloty/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright pandas pytz\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ts_utc gpu_type  gpu_count duration  usd_per_gpu_hr\n",
      "0   2025-09-10T19:31:20     H100          8   1 hour            1.40\n",
      "1   2025-09-10T19:31:20     H100         16   1 hour            1.41\n",
      "2   2025-09-10T19:31:20     H100         32   1 hour            1.39\n",
      "3   2025-09-10T19:31:20     H100         64   1 hour            1.40\n",
      "4   2025-09-10T19:31:20     H100        128   1 hour            1.44\n",
      "5   2025-09-10T19:31:20     H100        256   1 hour            1.55\n",
      "6   2025-09-10T19:31:20     H100          8    1 day            1.40\n",
      "7   2025-09-10T19:31:20     H100         16    1 day            1.40\n",
      "8   2025-09-10T19:31:20     H100         32    1 day            1.40\n",
      "9   2025-09-10T19:31:20     H100         64    1 day            1.40\n",
      "10  2025-09-10T19:31:20     H100        128    1 day            1.40\n",
      "11  2025-09-10T19:31:20     H100        256    1 day            1.77\n",
      "12  2025-09-10T19:31:20     H100          8   1 week            1.40\n",
      "13  2025-09-10T19:31:20     H100         16   1 week            1.40\n",
      "14  2025-09-10T19:31:20     H100         32   1 week            1.40\n",
      "15  2025-09-10T19:31:20     H100         64   1 week            1.40\n",
      "16  2025-09-10T19:31:20     H100        128   1 week            1.40\n",
      "17  2025-09-10T19:31:20     H100        256   1 week             NaN\n",
      "18  2025-09-10T19:31:20     H100          8  1 month            1.40\n",
      "19  2025-09-10T19:31:20     H100         16  1 month            1.40\n",
      "20  2025-09-10T19:31:20     H100         32  1 month            1.40\n",
      "21  2025-09-10T19:31:20     H100         64  1 month            1.40\n",
      "22  2025-09-10T19:31:20     H100        128  1 month            1.40\n",
      "23  2025-09-10T19:31:20     H100        256  1 month             NaN\n",
      "24  2025-09-10T19:31:20     H200          8   1 hour            1.99\n",
      "25  2025-09-10T19:31:20     H200         16   1 hour            2.01\n",
      "26  2025-09-10T19:31:20     H200         32   1 hour            2.01\n",
      "27  2025-09-10T19:31:20     H200         64   1 hour            2.01\n",
      "28  2025-09-10T19:31:20     H200        128   1 hour            1.99\n",
      "29  2025-09-10T19:31:20     H200        256   1 hour             NaN\n",
      "30  2025-09-10T19:31:20     H200          8    1 day            2.00\n",
      "31  2025-09-10T19:31:20     H200         16    1 day            2.00\n",
      "32  2025-09-10T19:31:20     H200         32    1 day            2.00\n",
      "33  2025-09-10T19:31:20     H200         64    1 day            2.00\n",
      "34  2025-09-10T19:31:20     H200        128    1 day            2.00\n",
      "35  2025-09-10T19:31:20     H200        256    1 day             NaN\n",
      "36  2025-09-10T19:31:20     H200          8   1 week            2.10\n",
      "37  2025-09-10T19:31:20     H200         16   1 week            2.00\n",
      "38  2025-09-10T19:31:20     H200         32   1 week            2.00\n",
      "39  2025-09-10T19:31:20     H200         64   1 week            2.00\n",
      "40  2025-09-10T19:31:20     H200        128   1 week            2.00\n",
      "41  2025-09-10T19:31:20     H200        256   1 week             NaN\n",
      "42  2025-09-10T19:31:20     H200          8  1 month            2.00\n",
      "43  2025-09-10T19:31:20     H200         16  1 month            2.00\n",
      "44  2025-09-10T19:31:20     H200         32  1 month            2.10\n",
      "45  2025-09-10T19:31:20     H200         64  1 month            2.00\n",
      "46  2025-09-10T19:31:20     H200        128  1 month            2.00\n",
      "47  2025-09-10T19:31:20     H200        256  1 month             NaN\n",
      "Saved: sfcompute_grid/grid_20250910.csv\n",
      "History file updated: sfcompute_grid/grid_history.csv\n"
     ]
    }
   ],
   "source": [
    "# SF Compute price grid (keep your flow; fix \"1 week\" + all counts)\n",
    "import re, nest_asyncio, asyncio, datetime as dt, pandas as pd\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "GPU_TYPES = [\"H100\", \"H200\"]\n",
    "DURATIONS_EXPECTED = [\"1 hour\", \"1 day\", \"1 week\", \"1 month\"]\n",
    "\n",
    "# Strict $-anchored price AND a decimal fallback (used only when $ not in text)\n",
    "PRICE_RE = re.compile(r\"(?<=\\$)\\s*([0-9]+(?:\\.[0-9]{1,3})?)\")\n",
    "DECIMAL_FALLBACK_RE = re.compile(r\"([0-9]+(?:\\.[0-9]{1,3}))\")\n",
    "\n",
    "\n",
    "async def _select_explore_prices_tab(page):\n",
    "    try:\n",
    "        await page.get_by_role(\"tab\", name=re.compile(r\"Explore Prices\", re.I)).click(timeout=1500)\n",
    "    except:\n",
    "        try:\n",
    "            await page.get_by_text(re.compile(r\"Explore Prices\", re.I)).first.click(timeout=1500)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "async def _select_gpu(page, gpu_label: str):\n",
    "    \"\"\"Robustly select GPU type (H100/H200).\"\"\"\n",
    "    # 1) Native <select>\n",
    "    try:\n",
    "        sel = page.get_by_role(\"combobox\").first\n",
    "        if await sel.count() > 0:\n",
    "            await sel.select_option(label=gpu_label)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    # 2) Custom dropdown\n",
    "    try:\n",
    "        dd = page.get_by_role(\"button\", name=re.compile(r\"H100|H200\", re.I)).first\n",
    "        if await dd.count() == 0:\n",
    "            dd = page.locator(\"div,button,[role='button']\").filter(has_text=re.compile(r\"H100|H200\")).first\n",
    "        await dd.click(timeout=1500)\n",
    "        await page.get_by_text(re.compile(rf\"^{gpu_label}$\", re.I)).first.click(timeout=1500)\n",
    "        return True\n",
    "    except:\n",
    "        pass\n",
    "    # 3) Button toggle\n",
    "    try:\n",
    "        await page.get_by_role(\"button\", name=re.compile(rf\"^{gpu_label}$\", re.I)).first.click(timeout=1500)\n",
    "        return True\n",
    "    except:\n",
    "        pass\n",
    "    # 4) Fallback\n",
    "    try:\n",
    "        await page.get_by_text(re.compile(rf\"^{gpu_label}$\", re.I)).first.click(timeout=1500)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "async def _wait_table_update(page, expected_gpu: str):\n",
    "    tbl = page.locator(\"table\").first\n",
    "    if await page.locator(\"table\").count() == 0:\n",
    "        tbl = page.locator(\"section, div\").filter(\n",
    "            has_text=re.compile(\"1 hour|1 day|1 week|1 month\", re.I)\n",
    "        ).first\n",
    "    try:\n",
    "        await tbl.wait_for(state=\"visible\", timeout=6000)\n",
    "    except:\n",
    "        pass\n",
    "    # settle after GPU switch so the row texts are fresh\n",
    "    await page.wait_for_timeout(400)\n",
    "    return tbl\n",
    "\n",
    "def _slice_one_row_text(full_text: str, dur: str) -> str:\n",
    "    \"\"\"Take only the text segment from this duration label up to the next duration label.\"\"\"\n",
    "    lo = full_text.lower()\n",
    "    start = lo.find(dur)\n",
    "    if start == -1:\n",
    "        return \"\"\n",
    "    next_pos = len(full_text)\n",
    "    for other in DURATIONS_EXPECTED:\n",
    "        if other == dur:\n",
    "            continue\n",
    "        p = lo.find(other, start + 1)\n",
    "        if p != -1:\n",
    "            next_pos = min(next_pos, p)\n",
    "    return full_text[start:next_pos]\n",
    "\n",
    "# ---------- NEW: DOM fallback for rows where '$' is CSS-injected (e.g., '1 week') ----------\n",
    "async def _extract_row_prices_via_dom(table_locator, dur: str, ncols: int):\n",
    "    \"\"\"\n",
    "    Find the visual row that contains `dur` and pull texts/attributes from its cells.\n",
    "    Returns a list of length ncols with floats or None.\n",
    "    \"\"\"\n",
    "    # locate the label element first\n",
    "    label_loc = table_locator.get_by_text(re.compile(rf\"\\b{re.escape(dur)}\\b\", re.I)).first\n",
    "    if await label_loc.count() == 0:\n",
    "        return None\n",
    "\n",
    "    # Grab candidate cell texts/attributes from the same row container\n",
    "    cell_texts = await label_loc.evaluate(\"\"\"\n",
    "      (el) => {\n",
    "        function findRowRoot(node){\n",
    "          while (node && node !== document.body) {\n",
    "            if (node.tagName === 'TR') return node;\n",
    "            const role = node.getAttribute && node.getAttribute('role');\n",
    "            if (role && role.toLowerCase() === 'row') return node;\n",
    "            const cls = (node.className || '').toString();\n",
    "            if (/\\\\brow\\\\b/i.test(cls)) return node;\n",
    "            node = node.parentElement;\n",
    "          }\n",
    "          return null;\n",
    "        }\n",
    "        const row = findRowRoot(el) || el.parentElement;\n",
    "        if (!row) return [];\n",
    "\n",
    "        // Prefer direct siblings after the label cell; otherwise, collect all leaf cells\n",
    "        const out = [];\n",
    "        const children = Array.from(row.children);\n",
    "        let startIdx = children.findIndex(ch => ch.contains(el));\n",
    "        if (startIdx >= 0) {\n",
    "          for (let i = startIdx + 1; i < children.length; i++) {\n",
    "            const c = children[i];\n",
    "            const t = (c.innerText || c.textContent || '').replace(/\\\\s+/g,' ').trim();\n",
    "            const aria = c.getAttribute && (c.getAttribute('aria-label') || '');\n",
    "            const data = c.getAttribute && (c.getAttribute('data-price') || '');\n",
    "            out.push([t, aria, data].filter(Boolean).join(' '));\n",
    "          }\n",
    "        }\n",
    "        if (out.length === 0) {\n",
    "          const cells = Array.from(row.querySelectorAll('td,th,a,button,div,span'));\n",
    "          for (const c of cells) {\n",
    "            if (c.contains(el) || el.contains(c)) continue;\n",
    "            const t = (c.innerText || c.textContent || '').replace(/\\\\s+/g,' ').trim();\n",
    "            const aria = c.getAttribute && (c.getAttribute('aria-label') || '');\n",
    "            const data = c.getAttribute && (c.getAttribute('data-price') || '');\n",
    "            out.push([t, aria, data].filter(Boolean).join(' '));\n",
    "          }\n",
    "        }\n",
    "        return out;\n",
    "      }\n",
    "    \"\"\")\n",
    "\n",
    "    # Parse decimals; require a decimal point to avoid '1' from '1 week'\n",
    "    vals = []\n",
    "    for txt in cell_texts:\n",
    "        m = DECIMAL_FALLBACK_RE.search(txt)\n",
    "        vals.append(float(m.group(1)) if m else None)\n",
    "        if len(vals) >= ncols:\n",
    "            break\n",
    "\n",
    "    if not any(v is not None for v in vals):\n",
    "        return None\n",
    "\n",
    "    # pad to ncols\n",
    "    return vals + [None] * (ncols - len(vals))\n",
    "\n",
    "# ---------- MODIFIED: parser that prefers $ text, then DOM fallback ----------\n",
    "async def _parse_grid_from_table(table_locator):\n",
    "    \"\"\"\n",
    "    Minimal-change parser:\n",
    "    - Force columns to the full set [8,16,32,64,128,256]\n",
    "    - For each duration label, grab that *row* from the DOM and parse the next cells left-to-right.\n",
    "    - Prefer $-anchored numbers; if missing (e.g., CSS-injected $ in 1-week links), fall back to decimals.\n",
    "    \"\"\"\n",
    "    counts_int = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "    rows = {}\n",
    "    for dur in DURATIONS_EXPECTED:\n",
    "        # Locate the row that contains exactly this duration label\n",
    "        row = table_locator.locator(\"tr\", has_text=re.compile(rf\"^\\s*{re.escape(dur)}\\s*$\", re.I)).first\n",
    "        if await row.count() == 0:\n",
    "            # Sometimes it's not a semantic <tr>; look for any row-like container\n",
    "            row = table_locator.locator(\"*[role='row'], div, section\").filter(\n",
    "                has_text=re.compile(rf\"^\\s*{re.escape(dur)}\\s*$\", re.I)\n",
    "            ).first\n",
    "\n",
    "        # Pull texts from the next N cells after the label cell\n",
    "        # We read both plain cell text and any nested link/button text/attributes.\n",
    "        cell_texts = await row.evaluate(\"\"\"\n",
    "          (el) => {\n",
    "            // Find the element that has the duration text, then read its siblings\n",
    "            const textMatches = (node, dur) =>\n",
    "              (node.innerText || node.textContent || '').trim().toLowerCase() === dur.toLowerCase();\n",
    "\n",
    "            function findLabelCell(root, dur){\n",
    "              const all = Array.from(root.querySelectorAll('th,td,div,span'));\n",
    "              return all.find(n => textMatches(n, dur)) || root;\n",
    "            }\n",
    "\n",
    "            function collectCellStrings(nodes){\n",
    "              const out = [];\n",
    "              for(const n of nodes){\n",
    "                const base = (n.innerText || n.textContent || '').replace(/\\\\s+/g,' ').trim();\n",
    "                const aria = n.getAttribute?.('aria-label') || '';\n",
    "                const title = n.getAttribute?.('title') || '';\n",
    "                const data  = n.getAttribute?.('data-price') || '';\n",
    "                // Include nested anchors/buttons in case the number lives there\n",
    "                const nested = Array.from(n.querySelectorAll('a,button,span,div')).map(x =>\n",
    "                  (x.innerText || x.textContent || '').replace(/\\\\s+/g,' ').trim()\n",
    "                ).filter(Boolean).join(' ');\n",
    "                out.push([base, aria, title, data, nested].filter(Boolean).join(' '));\n",
    "              }\n",
    "              return out;\n",
    "            }\n",
    "\n",
    "            const label = findLabelCell(el, /* dur injected below by Playwright */ '');\n",
    "            // Children after label within the same row/container\n",
    "            const siblings = Array.from(label.parentElement?.children || []);\n",
    "            const idx = siblings.indexOf(label);\n",
    "            const after = idx >= 0 ? siblings.slice(idx + 1) : [];\n",
    "\n",
    "            // If that yields nothing (non-tabular markup), fall back to all cells under row\n",
    "            const cells = after.length ? after : Array.from(el.querySelectorAll('td,th,div,span'));\n",
    "            return collectCellStrings(cells);\n",
    "          }\n",
    "        \"\"\", arg=dur)\n",
    "\n",
    "        # Now extract numbers from those cell strings\n",
    "        vals = []\n",
    "        for txt in cell_texts:\n",
    "            # 1) Try $-anchored prices first\n",
    "            m = PRICE_RE.search(txt)\n",
    "            if m:\n",
    "                vals.append(float(m.group(1)))\n",
    "            else:\n",
    "                # 2) Fallback to decimals (handles the 1-week link cells)\n",
    "                m2 = DECIMAL_FALLBACK_RE.search(txt)\n",
    "                vals.append(float(m2.group(1)) if m2 else None)\n",
    "            if len(vals) >= len(counts_int):\n",
    "                break\n",
    "\n",
    "        # Pad/trim to the expected 6 columns\n",
    "        vals = (vals + [None] * len(counts_int))[:len(counts_int)]\n",
    "        rows[dur] = vals\n",
    "\n",
    "    return counts_int, rows\n",
    "\n",
    "\n",
    "async def scrape_sfcompute_grid(headless=True, slow_mo=0):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless, slow_mo=slow_mo)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(\"https://sfcompute.com/buy\", wait_until=\"networkidle\")\n",
    "        await _select_explore_prices_tab(page)\n",
    "\n",
    "        all_rows = []\n",
    "        ts = dt.datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        for gpu in GPU_TYPES:\n",
    "            await _select_gpu(page, gpu)\n",
    "            table = await _wait_table_update(page, gpu)\n",
    "            counts_int, rows = await _parse_grid_from_table(table)\n",
    "\n",
    "            for dur, vals in rows.items():\n",
    "                for c, val in zip(counts_int, vals):\n",
    "                    all_rows.append({\n",
    "                        \"ts_utc\": ts,\n",
    "                        \"gpu_type\": gpu,\n",
    "                        \"gpu_count\": c,\n",
    "                        \"duration\": dur,\n",
    "                        \"usd_per_gpu_hr\": val\n",
    "                    })\n",
    "\n",
    "        await browser.close()\n",
    "        return pd.DataFrame(all_rows)\n",
    "\n",
    "# ---- Run & Save ----\n",
    "df = await scrape_sfcompute_grid(headless=True)\n",
    "print(df)\n",
    "\n",
    "out_dir = Path(\"sfcompute_grid\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "today = dt.datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "daily_file = out_dir / f\"grid_{today}.csv\"\n",
    "hist_file  = out_dir / \"grid_history.csv\"\n",
    "\n",
    "df.to_csv(daily_file, index=False)\n",
    "if hist_file.exists():\n",
    "    hist = pd.read_csv(hist_file)\n",
    "    hist = pd.concat([hist, df], ignore_index=True).drop_duplicates()\n",
    "    hist.to_csv(hist_file, index=False)\n",
    "else:\n",
    "    df.to_csv(hist_file, index=False)\n",
    "\n",
    "print(\"Saved:\", daily_file)\n",
    "print(\"History file updated:\", hist_file)\n",
    "#CORRECT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using history file: /Users/evieculloty/Documents/Forward Compute/sfcompute_history.csv\n",
      "Saving forecast to: /Users/evieculloty/Documents/Forward Compute/docs/data/sfcompute_forecast.csv\n",
      "Forecast generated for H100-1 day-8\n",
      "Forecast generated for H100-1 day-16\n",
      "Forecast generated for H100-1 day-32\n",
      "Forecast generated for H100-1 day-64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:33 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 day-128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 day-256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 hour-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 hour-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:39 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 hour-32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:39 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 hour-64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 hour-128\n",
      "Forecast generated for H100-1 hour-256\n",
      "Forecast generated for H100-1 month-8\n",
      "Forecast generated for H100-1 month-16\n",
      "Forecast generated for H100-1 month-32\n",
      "Forecast generated for H100-1 month-64\n",
      "Forecast generated for H100-1 month-128\n",
      "Skipping H100-1 month-256: not enough data (18)\n",
      "Forecast generated for H100-1 week-8\n",
      "Forecast generated for H100-1 week-16\n",
      "Forecast generated for H100-1 week-32\n",
      "Forecast generated for H100-1 week-64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H100-1 week-128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:45 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 day-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 day-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:45 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 day-32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:46 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 day-64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 day-128\n",
      "Skipping H200-1 day-256: not enough data (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 hour-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:39:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 hour-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:49 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 hour-32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:39:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:39:50 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 hour-64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:40:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:40:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 hour-128\n",
      "Skipping H200-1 hour-256: not enough data (14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:40:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 month-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:40:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 month-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 month-32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:40:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 month-64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:40:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 month-128\n",
      "Skipping H200-1 month-256: not enough data (14)\n",
      "Forecast generated for H200-1 week-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:40:19 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 week-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:40:20 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 week-32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:20 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast generated for H200-1 week-64\n",
      "Forecast generated for H200-1 week-128\n",
      "✅ Saved forecast → docs/data/sfcompute_forecast.csv (10938 rows)\n"
     ]
    }
   ],
   "source": [
    "# === Forecast Layer (Prophet) — works locally & on GitHub, fixes timezone ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from prophet import Prophet\n",
    "import datetime as dt\n",
    "\n",
    "# ---------- locate files (GitHub first, local fallback) ----------\n",
    "hist_path = Path(\"docs/data/sfcompute_history.csv\")\n",
    "if not hist_path.exists():\n",
    "    hist_path = Path(\"sfcompute_history.csv\")  # local testing fallback\n",
    "\n",
    "out_path = Path(\"docs/data/sfcompute_forecast.csv\")\n",
    "if not out_path.parent.exists():\n",
    "    out_path = Path(\"sfcompute_forecast.csv\")  # local fallback\n",
    "\n",
    "print(\"Using history file:\", hist_path.resolve())\n",
    "print(\"Saving forecast to:\", out_path.resolve())\n",
    "\n",
    "# ---------- load & clean ----------\n",
    "df = pd.read_csv(hist_path)\n",
    "# expected columns: ts_utc, gpu_type, duration, gpu_count, usd_per_gpu_hr\n",
    "df = df.dropna(subset=[\"ts_utc\", \"usd_per_gpu_hr\"])\n",
    "df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ts_utc\"])\n",
    "df = df.sort_values(\"ts_utc\")\n",
    "\n",
    "# Make a tz-naive UTC column for Prophet\n",
    "df[\"ds\"] = df[\"ts_utc\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "df[\"y\"]  = pd.to_numeric(df[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"y\"])\n",
    "\n",
    "# remove duplicates on the same timestamp per group (keep last)\n",
    "df = df.drop_duplicates(subset=[\"gpu_type\",\"duration\",\"gpu_count\",\"ds\"], keep=\"last\")\n",
    "\n",
    "# ---------- params ----------\n",
    "HORIZON_HOURS = 24 * 7     # 7-day hourly forecast\n",
    "MIN_POINTS    = 20         # skip groups with too little history\n",
    "TRAIN_CLIP_D  = 30         # (optional) only last 30 days of history to speed up\n",
    "\n",
    "results = []\n",
    "\n",
    "# group and forecast per combination\n",
    "for (gpu, dur, cnt), g in df.groupby([\"gpu_type\",\"duration\",\"gpu_count\"], dropna=False):\n",
    "    g = g[[\"ds\",\"y\"]].sort_values(\"ds\")\n",
    "\n",
    "    # optional: clip to recent history\n",
    "    if TRAIN_CLIP_D:\n",
    "        cutoff = g[\"ds\"].max() - pd.Timedelta(days=TRAIN_CLIP_D)\n",
    "        g = g[g[\"ds\"] >= cutoff]\n",
    "\n",
    "    if len(g) < MIN_POINTS:\n",
    "        print(f\"Skipping {gpu}-{dur}-{cnt}: not enough data ({len(g)})\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        m = Prophet(daily_seasonality=True, weekly_seasonality=True)\n",
    "        m.fit(g)\n",
    "\n",
    "        future = m.make_future_dataframe(periods=HORIZON_HOURS, freq=\"H\")\n",
    "        fc = m.predict(future)[[\"ds\",\"yhat\",\"yhat_lower\",\"yhat_upper\"]].copy()\n",
    "\n",
    "        fc[\"gpu_type\"] = gpu\n",
    "        fc[\"duration\"] = dur\n",
    "        fc[\"gpu_count\"] = cnt\n",
    "        fc[\"generated_utc\"] = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat()\n",
    "\n",
    "        results.append(fc)\n",
    "        print(f\"Forecast generated for {gpu}-{dur}-{cnt}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback: naïve constant forecast using last observed value\n",
    "        last_ds = g[\"ds\"].max()\n",
    "        last_y  = g.loc[g[\"ds\"].idxmax(), \"y\"]\n",
    "        future_idx = pd.date_range(last_ds + pd.Timedelta(hours=1), periods=HORIZON_HOURS, freq=\"H\")\n",
    "        fc = pd.DataFrame({\n",
    "            \"ds\": future_idx,\n",
    "            \"yhat\": last_y,\n",
    "            \"yhat_lower\": last_y * 0.99,\n",
    "            \"yhat_upper\": last_y * 1.01,\n",
    "            \"gpu_type\": gpu,\n",
    "            \"duration\": dur,\n",
    "            \"gpu_count\": cnt,\n",
    "            \"generated_utc\": dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat(),\n",
    "        })\n",
    "        results.append(fc)\n",
    "        print(f\"⚠️ Prophet failed for {gpu}-{dur}-{cnt}: {e} — wrote naïve fallback.\")\n",
    "\n",
    "# ---------- save ----------\n",
    "if results:\n",
    "    forecast_all = pd.concat(results, ignore_index=True)\n",
    "    # ensure ds is ISO string for the dashboard CSV\n",
    "    forecast_all[\"ds\"] = pd.to_datetime(forecast_all[\"ds\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    forecast_all.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Saved forecast → {out_path} ({len(forecast_all)} rows)\")\n",
    "else:\n",
    "    print(\"⚠️ No forecasts were generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAST metrics saved → docs/data/sfcompute_metrics.csv (12 groups, DO_BACKTEST=True)\n"
     ]
    }
   ],
   "source": [
    "# === FAST METRICS/BACKTEST (laptop-friendly) ===\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from prophet import Prophet\n",
    "import warnings, logging\n",
    "\n",
    "# quiet cmdstanpy/progress\n",
    "logging.getLogger(\"cmdstanpy\").disabled = True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----- settings (tune these for speed vs quality) -----\n",
    "DO_BACKTEST     = True      # set False to skip backtest entirely\n",
    "MAX_GROUPS      = 12        # cap number of groups to evaluate\n",
    "ROLL_WINDOW_H   = 24        # evaluate last 24 hours only\n",
    "STEP_HOURS      = 6         # fit every 6 hours (≈4 fits per group)\n",
    "MIN_POINTS      = 36        # need at least 36 points to run\n",
    "COUNTS_KEEP     = {8,16,32,64}\n",
    "DURS_KEEP       = {\"1 hour\",\"1 day\",\"1 week\"}\n",
    "TYPES_KEEP      = {\"H100\",\"H200\"}\n",
    "\n",
    "# ----- locate files -----\n",
    "hist_path = Path(\"docs/data/sfcompute_history.csv\")\n",
    "if not hist_path.exists(): hist_path = Path(\"sfcompute_history.csv\")\n",
    "metrics_out = Path(\"docs/data/sfcompute_metrics.csv\")\n",
    "if not metrics_out.parent.exists(): metrics_out = Path(\"sfcompute_metrics.csv\")\n",
    "\n",
    "# ----- load & prep -----\n",
    "df = pd.read_csv(hist_path)\n",
    "df = df.dropna(subset=[\"ts_utc\",\"usd_per_gpu_hr\"])\n",
    "df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True)\n",
    "df = df.sort_values(\"ts_utc\")\n",
    "\n",
    "# focus on most relevant groups to keep runtime small\n",
    "df = df[df[\"gpu_type\"].isin(TYPES_KEEP) & df[\"duration\"].isin(DURS_KEEP) & df[\"gpu_count\"].isin(COUNTS_KEEP)]\n",
    "\n",
    "# Prophet-friendly columns (tz-naive)\n",
    "df[\"ds\"] = df[\"ts_utc\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "df[\"y\"]  = pd.to_numeric(df[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"y\"])\n",
    "df = df.drop_duplicates(subset=[\"gpu_type\",\"duration\",\"gpu_count\",\"ds\"], keep=\"last\")\n",
    "\n",
    "# choose top groups by number of observations\n",
    "grp_sizes = df.groupby([\"gpu_type\",\"duration\",\"gpu_count\"]).size().sort_values(ascending=False)\n",
    "keep_groups = list(grp_sizes.index[:MAX_GROUPS])\n",
    "\n",
    "def smape(a, f):\n",
    "    a, f = np.array(a), np.array(f)\n",
    "    denom = (np.abs(a) + np.abs(f)) / 2\n",
    "    denom[denom==0] = 1e-9\n",
    "    return float(np.mean(np.abs(f - a) / denom) * 100)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for (gpu,dur,cnt) in keep_groups:\n",
    "    g = df[(df.gpu_type==gpu)&(df.duration==dur)&(df.gpu_count==cnt)][[\"ds\",\"y\"]].sort_values(\"ds\").reset_index(drop=True)\n",
    "    n = len(g)\n",
    "    if n < MIN_POINTS:\n",
    "        rows.append(dict(gpu_type=gpu,duration=dur,gpu_count=cnt,n=n,\n",
    "                         mae=np.nan,mape=np.nan,smape=np.nan,\n",
    "                         mae_naive=np.nan,skill=np.nan,pi_coverage=np.nan,\n",
    "                         ensemble_w=0.5))\n",
    "        continue\n",
    "\n",
    "    if not DO_BACKTEST:\n",
    "        # quick metrics using one fit + in-sample prediction (very fast, approximate)\n",
    "        m = Prophet(daily_seasonality=True, weekly_seasonality=True,\n",
    "                    growth='flat', n_changepoints=5, changepoint_prior_scale=0.05)\n",
    "        m.fit(g.rename(columns={\"ds\":\"ds\",\"y\":\"y\"}))\n",
    "        fc = m.predict(g[[\"ds\"]])\n",
    "        yhat = fc[\"yhat\"].to_numpy()\n",
    "        y    = g[\"y\"].to_numpy()\n",
    "        naive = np.r_[np.nan, y[:-1]]\n",
    "        mask = ~np.isnan(naive)\n",
    "        mae = np.mean(np.abs(yhat[mask]-y[mask]))\n",
    "        mae_nv = np.mean(np.abs(naive[mask]-y[mask]))\n",
    "        skill = 1 - mae/mae_nv if mae_nv>0 else np.nan\n",
    "        rows.append(dict(gpu_type=gpu,duration=dur,gpu_count=cnt,n=n,\n",
    "                         mae=mae, mape=float(np.mean(np.abs((yhat[mask]-y[mask]) / np.clip(y[mask],1e-9,None)))*100),\n",
    "                         smape=smape(y[mask], yhat[mask]), mae_naive=mae_nv,\n",
    "                         skill=skill, pi_coverage=np.nan, ensemble_w=max(0.0, min(1.0, 0.5 + 0.5*(skill if np.isfinite(skill) else 0)))))\n",
    "        continue\n",
    "\n",
    "    # true (but small) walk-forward over last ROLL_WINDOW_H hours, step STEP_HOURS\n",
    "    end_time   = g[\"ds\"].max()\n",
    "    start_time = end_time - pd.Timedelta(hours=ROLL_WINDOW_H)\n",
    "    g_recent = g[g[\"ds\"] >= start_time].copy()\n",
    "    if len(g_recent) < 8:   # too little in the recent slice\n",
    "        g_recent = g.tail(24).copy()\n",
    "\n",
    "    preds, lowers, uppers, actuals, naive_preds = [], [], [], [], []\n",
    "    # roll in steps\n",
    "    idxs = np.arange(g.index.get_indexer(g_recent.index)[0], len(g), STEP_HOURS)\n",
    "    idxs = idxs[idxs>0]  # need at least 1 train obs\n",
    "\n",
    "    for t in idxs:\n",
    "        train = g.iloc[:t].copy()\n",
    "        test  = g.iloc[t:t+1].copy()\n",
    "\n",
    "        m = Prophet(daily_seasonality=True, weekly_seasonality=True,\n",
    "                    growth='flat', n_changepoints=5, changepoint_prior_scale=0.05)\n",
    "        m.fit(train.rename(columns={\"ds\":\"ds\",\"y\":\"y\"}))\n",
    "        fc = m.predict(test[[\"ds\"]])\n",
    "\n",
    "        preds.append(float(fc[\"yhat\"].iloc[0]))\n",
    "        lowers.append(float(fc[\"yhat_lower\"].iloc[0]))\n",
    "        uppers.append(float(fc[\"yhat_upper\"].iloc[0]))\n",
    "        actuals.append(float(test[\"y\"].iloc[0]))\n",
    "        naive_preds.append(float(train[\"y\"].iloc[-1]))\n",
    "\n",
    "    actuals = np.array(actuals); preds = np.array(preds); naive = np.array(naive_preds)\n",
    "    mae      = float(np.mean(np.abs(preds - actuals)))\n",
    "    mape     = float(np.mean(np.abs((preds - actuals) / np.clip(actuals, 1e-9, None))) * 100)\n",
    "    smape_v  = smape(actuals, preds)\n",
    "    mae_nv   = float(np.mean(np.abs(naive - actuals)))\n",
    "    skill    = float(1.0 - (mae / mae_nv)) if mae_nv > 0 else np.nan\n",
    "    cover    = float(np.mean((actuals >= np.array(lowers)) & (actuals <= np.array(uppers)))) if len(actuals) else np.nan\n",
    "    ensemble_w = max(0.0, min(1.0, 0.5 + 0.5*(skill if np.isfinite(skill) else 0)))\n",
    "\n",
    "    rows.append(dict(gpu_type=gpu,duration=dur,gpu_count=cnt,n=n,\n",
    "                     mae=mae, mape=mape, smape=smape_v,\n",
    "                     mae_naive=mae_nv, skill=skill, pi_coverage=cover,\n",
    "                     ensemble_w=ensemble_w))\n",
    "\n",
    "metrics = pd.DataFrame(rows)\n",
    "metrics.to_csv(metrics_out, index=False)\n",
    "print(f\"✅ FAST metrics saved → {metrics_out} ({len(metrics)} groups, DO_BACKTEST={DO_BACKTEST})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blended forecast saved → docs/data/sfcompute_forecast.csv (10938 rows)\n"
     ]
    }
   ],
   "source": [
    "# === BLEND (final, bulletproof) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "KEYS = [\"gpu_type\",\"duration\",\"gpu_count\"]\n",
    "\n",
    "# --- paths ---\n",
    "fore_path = Path(\"docs/data/sfcompute_forecast.csv\")\n",
    "if not fore_path.exists(): fore_path = Path(\"sfcompute_forecast.csv\")\n",
    "\n",
    "hist_path = Path(\"docs/data/sfcompute_history.csv\")\n",
    "if not hist_path.exists(): hist_path = Path(\"sfcompute_history.csv\")\n",
    "\n",
    "met_path = Path(\"docs/data/sfcompute_metrics.csv\")\n",
    "if not met_path.exists(): met_path = Path(\"sfcompute_metrics.csv\")\n",
    "\n",
    "# --- load forecast & history ---\n",
    "fa = pd.read_csv(fore_path)\n",
    "df = pd.read_csv(hist_path)\n",
    "\n",
    "# normalise types on keys\n",
    "for k in (\"gpu_type\",\"duration\"):\n",
    "    fa[k] = fa[k].astype(str)\n",
    "    df[k] = df[k].astype(str)\n",
    "fa[\"gpu_count\"] = pd.to_numeric(fa[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"gpu_count\"] = pd.to_numeric(df[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# prep history\n",
    "df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ts_utc\",\"usd_per_gpu_hr\"]).sort_values(\"ts_utc\")\n",
    "df[\"ds\"] = df[\"ts_utc\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "df[\"y\"]  = pd.to_numeric(df[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"y\"])\n",
    "\n",
    "# --- weights from metrics (optional) ---\n",
    "if met_path.exists():\n",
    "    met = pd.read_csv(met_path)\n",
    "    for k in (\"gpu_type\",\"duration\"):\n",
    "        if k in met: met[k] = met[k].astype(str)\n",
    "    if \"gpu_count\" in met:\n",
    "        met[\"gpu_count\"] = pd.to_numeric(met[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"ensemble_w\" not in met.columns:\n",
    "        skill = pd.to_numeric(met.get(\"skill\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "        met[\"ensemble_w\"] = np.clip(0.5 + 0.5*skill, 0.0, 1.0)\n",
    "    w_df = met[KEYS + [\"ensemble_w\"]].dropna(subset=[\"gpu_count\"])\n",
    "else:\n",
    "    w_df = fa[KEYS].drop_duplicates().assign(ensemble_w=0.5)\n",
    "\n",
    "fa = fa.merge(w_df, on=KEYS, how=\"left\")\n",
    "if \"ensemble_w\" not in fa.columns:\n",
    "    fa[\"ensemble_w\"] = 0.5\n",
    "fa[\"ensemble_w\"] = pd.to_numeric(fa[\"ensemble_w\"], errors=\"coerce\").fillna(0.5)\n",
    "\n",
    "# --- last observed value per group (naïve) ---\n",
    "last_y = (df.sort_values(\"ds\")\n",
    "            .groupby(KEYS)[\"y\"].last().rename(\"y_last\").reset_index())\n",
    "fa = fa.merge(last_y, on=KEYS, how=\"left\")\n",
    "\n",
    "# --- recent bounds per group (last 48 samples) ---\n",
    "g = df.sort_values(\"ds\").groupby(KEYS)[\"y\"]\n",
    "bmin = g.apply(lambda s: float(s.tail(48).min()) if len(s) else np.nan).rename(\"recent_min\").reset_index()\n",
    "bmax = g.apply(lambda s: float(s.tail(48).max()) if len(s) else np.nan).rename(\"recent_max\").reset_index()\n",
    "bounds = bmin.merge(bmax, on=KEYS, how=\"outer\")\n",
    "fa = fa.merge(bounds, on=KEYS, how=\"left\")\n",
    "\n",
    "# --- ensure columns exist before fill (prevents KeyErrors) ---\n",
    "for col in [\"y_last\",\"recent_min\",\"recent_max\"]:\n",
    "    if col not in fa.columns:\n",
    "        fa[col] = np.nan\n",
    "\n",
    "# --- fill fallbacks ---\n",
    "fa[\"y_last\"] = pd.to_numeric(fa[\"y_last\"], errors=\"coerce\")\n",
    "fa[\"y_last\"].fillna(pd.to_numeric(fa[\"yhat\"], errors=\"coerce\"), inplace=True)\n",
    "\n",
    "fa[\"recent_min\"] = pd.to_numeric(fa[\"recent_min\"], errors=\"coerce\")\n",
    "fa[\"recent_max\"] = pd.to_numeric(fa[\"recent_max\"], errors=\"coerce\")\n",
    "fa[\"recent_min\"].fillna(fa[\"y_last\"]*0.99, inplace=True)\n",
    "fa[\"recent_max\"].fillna(fa[\"y_last\"]*1.01, inplace=True)\n",
    "\n",
    "# --- blend + clip ---\n",
    "yhat_num = pd.to_numeric(fa[\"yhat\"], errors=\"coerce\")\n",
    "fa[\"yhat\"] = (fa[\"ensemble_w\"]*yhat_num + (1-fa[\"ensemble_w\"])*fa[\"y_last\"]).clip(\n",
    "    lower=fa[\"recent_min\"]*0.98, upper=fa[\"recent_max\"]*1.02\n",
    ")\n",
    "\n",
    "# --- save (overwrite) ---\n",
    "fa.drop(columns=[\"y_last\",\"recent_min\",\"recent_max\"], inplace=True, errors=\"ignore\")\n",
    "fa.to_csv(fore_path, index=False)\n",
    "print(f\"✅ Blended forecast saved → {fore_path} ({len(fa)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ROI catalog written → docs/data/roi_catalog.csv (144 rows)\n"
     ]
    }
   ],
   "source": [
    "# === ROI / Cost Catalog (from latest prices) ===\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIRS = [Path(\"docs/data\"), Path(\".\")]\n",
    "def first_existing(*names):\n",
    "    for d in DATA_DIRS:\n",
    "        for n in names:\n",
    "            p = d / n\n",
    "            if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "hist_path = first_existing(\"sfcompute_history.csv\", \"sfcompute_history.parquet\")\n",
    "latest_path = first_existing(\"sfcompute_latest.csv\")\n",
    "out_path = (Path(\"docs/data\") if (Path(\"docs\")).exists() else Path(\".\")) / \"roi_catalog.csv\"\n",
    "\n",
    "# Load latest; fallback to last row per group from history\n",
    "if latest_path:\n",
    "    latest = pd.read_csv(latest_path)\n",
    "    # expected: ts_utc, gpu_type, duration, gpu_count, usd_per_gpu_hr\n",
    "else:\n",
    "    df = pd.read_csv(hist_path)\n",
    "    df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True)\n",
    "    latest = (df.sort_values(\"ts_utc\")\n",
    "                .groupby([\"gpu_type\",\"duration\",\"gpu_count\"], as_index=False)\n",
    "                .tail(1)\n",
    "                [[\"gpu_type\",\"duration\",\"gpu_count\",\"usd_per_gpu_hr\",\"ts_utc\"]]\n",
    "                .rename(columns={\"ts_utc\":\"asof_utc\"}))\n",
    "\n",
    "# Normalise and compute hours per duration\n",
    "roi = latest.copy()\n",
    "roi[\"gpu_count\"] = pd.to_numeric(roi[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "roi[\"usd_per_gpu_hr\"] = pd.to_numeric(roi[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "\n",
    "HOURS_PER_MONTH = 730  # standard billing month\n",
    "HOURS_BY_DURATION = {\n",
    "    \"1 hour\": 1,\n",
    "    \"1 day\": 24,\n",
    "    \"1 week\": 7*24,\n",
    "    \"1 month\": HOURS_PER_MONTH\n",
    "}\n",
    "\n",
    "roi[\"hours_per_unit\"] = roi[\"duration\"].map(HOURS_BY_DURATION).fillna(1)\n",
    "roi[\"monthly_hours\"]  = HOURS_PER_MONTH\n",
    "# monthly cost for a given fleet size N is: price/hr * monthly_hours * gpu_count_multiplier\n",
    "roi[\"monthly_cost_usd_at_100pct_util\"] = roi[\"usd_per_gpu_hr\"] * roi[\"monthly_hours\"] * roi[\"gpu_count\"]\n",
    "\n",
    "# add a few standard utilisation presets the UI can pick from (50/75/100%)\n",
    "out_rows = []\n",
    "for u in (0.5, 0.75, 1.0):\n",
    "    tmp = roi.copy()\n",
    "    tmp[\"utilisation\"] = u\n",
    "    tmp[\"monthly_cost_usd\"] = tmp[\"monthly_cost_usd_at_100pct_util\"] * u\n",
    "    out_rows.append(tmp)\n",
    "\n",
    "roi_out = pd.concat(out_rows, ignore_index=True)\n",
    "roi_out = roi_out[[\"gpu_type\",\"duration\",\"gpu_count\",\"usd_per_gpu_hr\",\"utilisation\",\"monthly_cost_usd\",\"monthly_hours\"]]\n",
    "roi_out.to_csv(out_path, index=False)\n",
    "print(f\"✅ ROI catalog written → {out_path} ({len(roi_out)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Latest price table → docs/data/price_table_latest.csv (48 rows)\n",
      "✅ Budget scenarios → docs/data/budget_scenarios.csv (3 rows)\n"
     ]
    }
   ],
   "source": [
    "# === Budget Simulation tables (latest price table + example scenarios) ===\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIRS = [Path(\"docs/data\"), Path(\".\")]\n",
    "def first_existing(*names):\n",
    "    for d in DATA_DIRS:\n",
    "        for n in names:\n",
    "            p = d / n\n",
    "            if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "latest_path = first_existing(\"sfcompute_latest.csv\")\n",
    "hist_path   = first_existing(\"sfcompute_history.csv\")\n",
    "out_prices  = (Path(\"docs/data\") if (Path(\"docs\")).exists() else Path(\".\")) / \"price_table_latest.csv\"\n",
    "out_sims    = (Path(\"docs/data\") if (Path(\"docs\")).exists() else Path(\".\")) / \"budget_scenarios.csv\"\n",
    "\n",
    "if latest_path:\n",
    "    price_tbl = pd.read_csv(latest_path)\n",
    "else:\n",
    "    df = pd.read_csv(hist_path)\n",
    "    df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True)\n",
    "    price_tbl = (df.sort_values(\"ts_utc\")\n",
    "                   .groupby([\"gpu_type\",\"duration\",\"gpu_count\"], as_index=False)\n",
    "                   .tail(1)\n",
    "                   [[\"gpu_type\",\"duration\",\"gpu_count\",\"usd_per_gpu_hr\",\"ts_utc\"]]\n",
    "                   .rename(columns={\"ts_utc\":\"asof_utc\"}))\n",
    "\n",
    "price_tbl[\"gpu_count\"] = pd.to_numeric(price_tbl[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "price_tbl[\"usd_per_gpu_hr\"] = pd.to_numeric(price_tbl[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "price_tbl.to_csv(out_prices, index=False)\n",
    "print(f\"✅ Latest price table → {out_prices} ({len(price_tbl)} rows)\")\n",
    "\n",
    "# Example budget scenarios the UI can show (edit freely)\n",
    "scenarios = pd.DataFrame([\n",
    "    {\"label\":\"H100 • 8 GPUs • 1h (spot) • 100% util\",  \"gpu_type\":\"H100\",\"gpu_count\":8,\"duration\":\"1 hour\",\"util\":1.0},\n",
    "    {\"label\":\"H100 • 128 GPUs • 1w (reserved) • 80%\",  \"gpu_type\":\"H100\",\"gpu_count\":128,\"duration\":\"1 week\",\"util\":0.8},\n",
    "    {\"label\":\"H200 • 32 GPUs • 1d (reserved) • 70%\",   \"gpu_type\":\"H200\",\"gpu_count\":32,\"duration\":\"1 day\",\"util\":0.7},\n",
    "])\n",
    "\n",
    "HOURS_PER_MONTH = 730\n",
    "sim = scenarios.merge(price_tbl, on=[\"gpu_type\",\"duration\",\"gpu_count\"], how=\"left\")\n",
    "sim[\"usd_per_gpu_hr\"] = pd.to_numeric(sim[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "sim[\"monthly_cost_usd\"] = sim[\"usd_per_gpu_hr\"] * HOURS_PER_MONTH * sim[\"gpu_count\"] * sim[\"util\"]\n",
    "sim = sim[[\"label\",\"gpu_type\",\"duration\",\"gpu_count\",\"util\",\"usd_per_gpu_hr\",\"monthly_cost_usd\"]]\n",
    "sim.to_csv(out_sims, index=False)\n",
    "print(f\"✅ Budget scenarios → {out_sims} ({len(sim)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Alerts written → docs/data/alerts.csv (0 rows)\n"
     ]
    }
   ],
   "source": [
    "# === Price Alerts (uses competitor median if available; else 7d SF rolling median) ===\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIRS = [Path(\"docs/data\"), Path(\".\")]\n",
    "def first_existing(*names):\n",
    "    for d in DATA_DIRS:\n",
    "        for n in names:\n",
    "            p = d / n\n",
    "            if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "latest_path   = first_existing(\"sfcompute_latest.csv\")\n",
    "hist_path     = first_existing(\"sfcompute_history.csv\")\n",
    "bench_path    = first_existing(\"competitor_median.csv\")  # optional\n",
    "out_alerts    = (Path(\"docs/data\") if (Path(\"docs\")).exists() else Path(\".\")) / \"alerts.csv\"\n",
    "THRESH = 0.05  # 5% below benchmark\n",
    "\n",
    "# load latest\n",
    "if latest_path:\n",
    "    latest = pd.read_csv(latest_path)\n",
    "else:\n",
    "    df = pd.read_csv(hist_path)\n",
    "    df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True)\n",
    "    latest = (df.sort_values(\"ts_utc\")\n",
    "                .groupby([\"gpu_type\",\"duration\",\"gpu_count\"], as_index=False)\n",
    "                .tail(1)\n",
    "                [[\"gpu_type\",\"duration\",\"gpu_count\",\"usd_per_gpu_hr\",\"ts_utc\"]]\n",
    "                .rename(columns={\"ts_utc\":\"asof_utc\"}))\n",
    "\n",
    "latest[\"gpu_count\"] = pd.to_numeric(latest[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "latest[\"usd_per_gpu_hr\"] = pd.to_numeric(latest[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "\n",
    "# build benchmark\n",
    "if bench_path:\n",
    "    bench = pd.read_csv(bench_path)\n",
    "    for k in (\"gpu_type\",\"duration\"): bench[k] = bench[k].astype(str)\n",
    "    bench[\"gpu_count\"] = pd.to_numeric(bench[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    bench = bench.rename(columns={\"median_price_hourly\":\"benchmark_price\"})\n",
    "else:\n",
    "    # internal benchmark = 7d rolling median of SF prices per group\n",
    "    df = pd.read_csv(hist_path)\n",
    "    df[\"ts_utc\"] = pd.to_datetime(df[\"ts_utc\"], utc=True)\n",
    "    df = df.sort_values(\"ts_utc\")\n",
    "    df[\"usd_per_gpu_hr\"] = pd.to_numeric(df[\"usd_per_gpu_hr\"], errors=\"coerce\")\n",
    "\n",
    "    # approx 7d window in hours; if irregular, use last N=168 points\n",
    "    def last7d_median(g):\n",
    "        s = g[\"usd_per_gpu_hr\"].tail(168)\n",
    "        return pd.Series({\"benchmark_price\": s.median()})\n",
    "\n",
    "    bench = (df.groupby([\"gpu_type\",\"duration\",\"gpu_count\"])\n",
    "               .apply(last7d_median)\n",
    "               .reset_index())\n",
    "    for k in (\"gpu_type\",\"duration\"): bench[k] = bench[k].astype(str)\n",
    "    bench[\"gpu_count\"] = pd.to_numeric(bench[\"gpu_count\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "alerts = latest.merge(bench, on=[\"gpu_type\",\"duration\",\"gpu_count\"], how=\"left\")\n",
    "alerts[\"delta_pct\"] = (alerts[\"usd_per_gpu_hr\"] - alerts[\"benchmark_price\"]) / alerts[\"benchmark_price\"]\n",
    "alerts[\"is_below_benchmark\"] = alerts[\"delta_pct\"] <= -THRESH\n",
    "\n",
    "alerts_out = alerts[alerts[\"is_below_benchmark\"]==True].copy()\n",
    "alerts_out = alerts_out[[\"gpu_type\",\"duration\",\"gpu_count\",\"usd_per_gpu_hr\",\"benchmark_price\",\"delta_pct\"]]\n",
    "alerts_out.to_csv(out_alerts, index=False)\n",
    "print(f\"✅ Alerts written → {out_alerts} ({len(alerts_out)} rows)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
